{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code below adapted from [Kaggle](https://www.kaggle.com/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign) purely for the sake of reading data\n",
    "### Will be demarcated where project code begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependencies\n",
    "# !pip install tensorflow, matplotlib, pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import os\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import time\n",
    "import itertools\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import tqdm\n",
    "import math\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline\n",
    "plotly.offline.init_notebook_mode(True)\n",
    "\n",
    "dataset_dir = '../data/'\n",
    "meta_info = os.path.join(dataset_dir, 'Meta.csv')\n",
    "train_csv_path = os.path.join(dataset_dir, 'Train.csv')\n",
    "test_csv_path = os.path.join(dataset_dir, 'Test.csv')\n",
    "labels = ['20 km/h', '30 km/h', '50 km/h', '60 km/h', '70 km/h', '80 km/h', '80 km/h end', '100 km/h', '120 km/h', 'No overtaking',\n",
    "               'No overtaking for tracks', 'Crossroad with secondary way', 'Main road', 'Give way', 'Stop', 'Road up', 'Road up for track', 'Brock',\n",
    "               'Other dangerous', 'Turn left', 'Turn right', 'Winding road', 'Hollow road', 'Slippery road', 'Narrowing road', 'Roadwork', 'Traffic light',\n",
    "               'Pedestrian', 'Children', 'Bike', 'Snow', 'Deer', 'End of the limits', 'Only right', 'Only left', 'Only straight', 'Only straight and right', \n",
    "               'Only straight and left', 'Take right', 'Take left', 'Circle crossroad', 'End of overtaking limit', 'End of overtaking limit for track']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "We should remap path because of kaggle converts folder to lowercase mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_color = '#0f7b8e'\n",
    "test_data_color = '#630f8e'\n",
    "\n",
    "trainDf = pd.read_csv(train_csv_path)\n",
    "testDf = pd.read_csv(test_csv_path)\n",
    "metaDf = pd.read_csv(meta_info)\n",
    "\n",
    "trainDf['Path'] = list(map(lambda x: os.path.join(dataset_dir,x.lower()), trainDf['Path']))\n",
    "testDf['Path'] = list(map(lambda x: os.path.join(dataset_dir,x.lower()), testDf['Path']))\n",
    "metaDf['Path'] = list(map(lambda x: os.path.join(dataset_dir,x.lower()), metaDf['Path']))\n",
    "\n",
    "trainDf.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover dataset balance\n",
    "The easiest way to discover dataset balance - build histogram. We consider to use [seaborn](https://seaborn.pydata.org/) library based on matplotlib for pretty data visualization.\n",
    "\n",
    "Train and test subset of dataset have similar balance distribution. Train and test split provided by GTSRB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(25, 6))\n",
    "axs[0].set_title('Train classes distribution')\n",
    "axs[0].set_xlabel('Class')\n",
    "axs[0].set_ylabel('Count')\n",
    "axs[1].set_title('Test classes distribution')\n",
    "axs[1].set_xlabel('Class')\n",
    "axs[1].set_ylabel('Count')\n",
    "\n",
    "sns.countplot(trainDf.ClassId, ax=axs[0])\n",
    "sns.countplot(testDf.ClassId, ax=axs[1])\n",
    "axs[0].set_xlabel('Class ID');\n",
    "axs[1].set_xlabel('Class ID');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image size distribution\n",
    "Dataset contains thouthands of images. Images don't have the same resolution. Some of them are big, other are small. We should somehow choose appropriate resolution of samples. The best way to visualize width and height corellation - using multivariate plotting.\n",
    "\n",
    "As we can see bellow, most of images is rectangular (it can be prooved by applying liniar regression on the samples resolution). Most of samples are about 35x35 pixels. And only few samples have big resolution like a 100x100 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDfDpiSubset = trainDf[(trainDf.Width < 80) & (trainDf.Height < 80)];\n",
    "testDfDpiSubset = testDf[(testDf.Width < 80) & (testDf.Height < 80)];\n",
    "\n",
    "g = sns.JointGrid(x=\"Width\", y=\"Height\", data=trainDfDpiSubset)\n",
    "sns.kdeplot(trainDfDpiSubset.Width, trainDfDpiSubset.Height, cmap=\"Reds\",\n",
    "        shade=False, shade_lowest=False, ax=g.ax_joint)\n",
    "sns.kdeplot(testDfDpiSubset.Width, testDfDpiSubset.Height, cmap=\"Blues\",\n",
    "        shade=False, shade_lowest=False, ax=g.ax_joint)\n",
    "sns.distplot(trainDfDpiSubset.Width, kde=True, hist=False, color=\"r\", ax=g.ax_marg_x, label='Train distribution')\n",
    "sns.distplot(testDfDpiSubset.Width, kde=True, hist=False, color=\"b\", ax=g.ax_marg_x, label='Test distribution')\n",
    "sns.distplot(trainDfDpiSubset.Width, kde=True, hist=False, color=\"r\", ax=g.ax_marg_y, vertical=True)\n",
    "sns.distplot(testDfDpiSubset.Height, kde=True, hist=False, color=\"b\", ax=g.ax_marg_y, vertical=True)\n",
    "g.fig.set_figwidth(25)\n",
    "g.fig.set_figheight(8)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target class visualization\n",
    "It is not a sample in the dataset, it is just a picture of sign. Some of them may be different from the dataset samples because of dataset contains images of German traffic signs and pictures bellow are Ukrainian traffic signs ([source of pictures](http://pdd.ua/33/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style()\n",
    "rows = 6\n",
    "cols = 8\n",
    "fig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(25, 12))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=0.9, wspace=None, hspace=None)\n",
    "metaDf = metaDf.sort_values(by=['ClassId'])\n",
    "\n",
    "idx = 0\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        if idx > 42:\n",
    "            break\n",
    "            \n",
    "        img = cv2.imread(metaDf[\"Path\"].tolist()[idx], cv2.IMREAD_UNCHANGED)\n",
    "        img[np.where(img[:,:,3]==0)] = [255,255,255,255]\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (60,60))\n",
    "        \n",
    "        axs[i,j].imshow(img)\n",
    "        axs[i,j].set_facecolor('xkcd:salmon')\n",
    "        axs[i,j].set_facecolor((1.0, 0.47, 0.42))\n",
    "        axs[i,j].set_title(labels[int(metaDf[\"ClassId\"].tolist()[idx])])\n",
    "        axs[i,j].get_xaxis().set_visible(False)\n",
    "        axs[i,j].get_yaxis().set_visible(False)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples visualization\n",
    "\n",
    "It is good idea to visualize samples in order to brief data exploration. Image visualization can help to understand data problem. Some solutions (such as histogram equalization) can be discovered by visual data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 6\n",
    "cols = 8+4\n",
    "fig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(25, 12))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=0.9, wspace=None, hspace=None)\n",
    "visualize = trainDf.sample(rows*cols)\n",
    "\n",
    "idx = 0\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        img = cv2.imread(visualize[\"Path\"].tolist()[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (60,60))\n",
    "        axs[i,j].imshow(img)\n",
    "        axs[i,j].set_title(labels[int(visualize[\"ClassId\"].tolist()[idx])])\n",
    "        axs[i,j].get_xaxis().set_visible(False)\n",
    "        axs[i,j].get_yaxis().set_visible(False)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow utils\n",
    "\n",
    "We decide to use native tensorflow as deep learning framework without high level API. The best way to implement data pipleline - implement load data operation as node of comoutition graph. This will be the first node of computitional graph (i.e. all other node is depended from it). This approach has several adantages:\n",
    "\n",
    "- Parallelism out of the box (provided by dataflow graph principies)\n",
    "- Opportunity to implement data augmentation on GPU (not implemented here)\n",
    "- Convenient way to wrok with it provided by tf.data infrastructure\n",
    "- Fast way to use native and 3rd party python libraries wrapped by tf.py_func (deprecated since 1.13.? version)\n",
    "\n",
    "Here we defined utils for loading tensors, augmentation and other data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_load_size = (60,60)\n",
    "zero_img = np.zeros([12,img_load_size[0], img_load_size[1], 3])\n",
    "zero_label = np.zeros([12,1])\n",
    "\n",
    "def parse_function(filename, label):\n",
    "        image_string = tf.read_file(filename)\n",
    "        image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "#         image = tf.py_func(eq, [image], image.dtype)\n",
    "        image.set_shape([None, None, 3])\n",
    "        \n",
    "        return filename, image, label\n",
    "    \n",
    "def train_preprocess(filename, image, label):\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize_images(image, img_load_size)\n",
    "    return filename, image, label\n",
    "\n",
    "def augmentate(filename, image, label):\n",
    "    grad = tf.random.uniform(shape=[], minval=-0.3, maxval=0.3)\n",
    "    dx = tf.random.uniform(shape=[], minval=-15, maxval=15, dtype=tf.int32)\n",
    "    dy = tf.random.uniform(shape=[], minval=-15, maxval=15, dtype=tf.int32)\n",
    "    image = tf.contrib.image.rotate(image, grad)\n",
    "    image = tf.contrib.image.translate(image, translations=[dx, dy])\n",
    "    \n",
    "    return filename, image, label\n",
    "\n",
    "def eq(img: np.ndarray):\n",
    "    res = img.copy()\n",
    "    res[:, :, 0] = cv2.equalizeHist(img[:, :, 0])\n",
    "    res[:, :, 1] = cv2.equalizeHist(img[:, :, 1])\n",
    "    res[:, :, 2] = cv2.equalizeHist(img[:, :, 2])\n",
    "    \n",
    "    return res\n",
    "\n",
    "def tf_equalize_histogram(image):\n",
    "    values_range = tf.constant([0., 255.], dtype = tf.float32)\n",
    "    histogram = tf.histogram_fixed_width(tf.to_float(image), values_range, 256)\n",
    "    cdf = tf.cumsum(histogram)\n",
    "    cdf_min = cdf[tf.reduce_min(tf.where(tf.greater(cdf, 0)))]\n",
    "\n",
    "    img_shape = tf.shape(image)\n",
    "    pix_cnt = img_shape[-3] * img_shape[-2]\n",
    "    px_map = tf.round(tf.to_float(cdf - cdf_min) * 255. / tf.to_float(pix_cnt - 1))\n",
    "    px_map = tf.cast(px_map, tf.uint8)\n",
    "\n",
    "    gth = tf.gather_nd(px_map, tf.cast(image, tf.int32))\n",
    "    eq_hist = tf.expand_dims(gth, 2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow data pipeline\n",
    "\n",
    "Above we have prepared pandas DataFrame with all data. Now, let's create operaition for loading data using [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 12\n",
    "prefetch_count = 1\n",
    "samples_train = len(trainDf)\n",
    "samples_test = len(testDf)\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((trainDf['Path'], trainDf['ClassId']))\n",
    "dataset_train = dataset_train.shuffle(len(trainDf['Path']))\n",
    "dataset_train = dataset_train.repeat(epochs)\n",
    "dataset_train = dataset_train.map(parse_function, num_parallel_calls=4)\n",
    "dataset_train = dataset_train.map(train_preprocess, num_parallel_calls=4)\n",
    "dataset_train = dataset_train.map(augmentate, num_parallel_calls=4)\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_train = dataset_train.prefetch(prefetch_count)\n",
    "\n",
    "dataset_iterator = tf.data.Iterator.from_structure(dataset_train.output_types,\n",
    "                                                          dataset_train.output_shapes)\n",
    "\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((testDf['Path'], testDf['ClassId']))\n",
    "dataset_test = dataset_test.shuffle(len(testDf['Path']))\n",
    "dataset_test = dataset_test.repeat(epochs+1)\n",
    "dataset_test = dataset_test.map(parse_function, num_parallel_calls=4)\n",
    "dataset_test = dataset_test.map(train_preprocess, num_parallel_calls=4)\n",
    "dataset_test = dataset_test.batch(batch_size)\n",
    "dataset_test = dataset_test.prefetch(prefetch_count)\n",
    "\n",
    "\n",
    "train_init_op = dataset_iterator.make_initializer(dataset_train)\n",
    "test_init_op = dataset_iterator.make_initializer(dataset_test)\n",
    "\n",
    "load_filename, load_img, load_label = dataset_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=8, nrows=1, figsize=(15, 6))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(train_init_op)\n",
    "    for j in range(8):\n",
    "        i, l = sess.run([load_img, load_label])\n",
    "        i = (i[0]*255).astype(np.uint8)\n",
    "        ax[j].imshow(i)\n",
    "        ax[j].set_title(labels[l[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "Model graph defined using native tensorflow API. Also, we should defined some placeholders in order to have opportunity to load custom controled data into netwrok (for further model analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_rate = tf.placeholder(dtype=tf.float32, shape=[], name='dp_rate')\n",
    "\n",
    "img_placeholder = tf.placeholder(shape=[None, 60,60,3], dtype=tf.float32, name='img_placeholder')\n",
    "label_placeholder = tf.placeholder(shape=[None, 1], dtype=tf.int64, name='label_placeholder')\n",
    "manual_load = tf.placeholder(dtype=tf.bool, shape=[], name='manual_load_placeholder')\n",
    "\n",
    "# inp = net = tf.cond(pred=manual_load, true_fn=lambda : img_placeholder, false_fn=lambda : load_img, name='network_start')\n",
    "# label = tf.cond(pred=manual_load, true_fn=lambda : label_placeholder, false_fn=lambda : load_label, name='label')\n",
    "\n",
    "inp = net = tf.cond(manual_load, lambda: img_placeholder, lambda: load_img)\n",
    "label = load_label\n",
    "\n",
    "conv1 = net = tf.layers.conv2d(inputs=net, filters=16, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\n",
    "net = tf.layers.batch_normalization(inputs=net)\n",
    "conv2 = net = tf.layers.conv2d(inputs=net, filters=32, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\n",
    "net = tf.layers.batch_normalization(inputs=net)\n",
    "\n",
    "conv3 = net = tf.layers.conv2d(inputs=net, filters=32, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\n",
    "net = tf.layers.batch_normalization(inputs=net)\n",
    "conv4 = net = tf.layers.conv2d(inputs=net, filters=64, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\n",
    "net = tf.layers.batch_normalization(inputs=net)\n",
    "\n",
    "net = tf.layers.max_pooling2d(inputs=net, pool_size=(2,2), strides=(2,2))\n",
    "\n",
    "conv5 = net = tf.layers.conv2d(inputs=net, filters=64, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\n",
    "net = tf.layers.batch_normalization(inputs=net)\n",
    "conv6 = net = tf.layers.conv2d(inputs=net, filters=128, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\n",
    "net = tf.layers.batch_normalization(inputs=net)\n",
    "\n",
    "conv5 = net = tf.layers.conv2d(inputs=net, filters=256, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\n",
    "net = tf.layers.batch_normalization(inputs=net)\n",
    "conv6 = net = tf.layers.conv2d(inputs=net, filters=400, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\n",
    "net = tf.layers.batch_normalization(inputs=net)\n",
    "\n",
    "flatten1 = net = tf.layers.flatten(inputs=net)\n",
    "\n",
    "dp1 = net = tf.layers.dropout(inputs=net, rate=dp_rate)\n",
    "dense1 = net = tf.layers.dense(inputs=net, units=256)\n",
    "logits = tf.layers.dense(inputs=net, units=43)\n",
    "\n",
    "pred_classes = tf.argmax(logits, axis=1)\n",
    "pred_probas = tf.nn.softmax(logits)\n",
    "\n",
    "acc, acc_op = tf.metrics.accuracy(labels=label, predictions=pred_classes)\n",
    "end_loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=label)\n",
    "\n",
    "loss = end_loss\n",
    "\n",
    "label_transpose = tf.transpose(label)\n",
    "correct_prediction = tf.equal(pred_classes, label_transpose)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "confusion_matrix_op = tf.confusion_matrix(labels=label, predictions=pred_classes, num_classes=43)\n",
    "\n",
    "opt = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "Train model several epoch and store results of each epoch for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# irn.load_weights('inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "train_history = {'loss':[], 'acc':[], 'val_loss':[], 'val_acc':[]}\n",
    "best_acc = 0\n",
    "\n",
    "for e in range(epochs):\n",
    "    epoch_history = {'loss':[], 'acc':[], 'val_loss':[], 'val_acc':[]}\n",
    "    \n",
    "    sess.run(train_init_op)\n",
    "    for i in tqdm.tqdm_notebook(range(samples_train//batch_size), ascii=True, desc='Train epoch {}'.format(e)):\n",
    "        _, _loss, _acc, mn = sess.run([opt, loss, accuracy, inp], feed_dict={dp_rate: 0.3, manual_load: False, img_placeholder: zero_img, label_placeholder: zero_label})\n",
    "#         print(np.mean(mn))\n",
    "        epoch_history['loss'].append(_loss)\n",
    "        epoch_history['acc'].append(_acc)\n",
    "        \n",
    "    sess.run(test_init_op)\n",
    "    for i in tqdm.tqdm_notebook(range(samples_test//batch_size), ascii=True, desc='Test epoch {}'.format(e)):\n",
    "        _loss, _acc = sess.run([loss, accuracy], feed_dict={dp_rate: 0, manual_load: False, img_placeholder: zero_img, label_placeholder: zero_label})\n",
    "        epoch_history['val_loss'].append(_loss)\n",
    "        epoch_history['val_acc'].append(_acc)\n",
    "        \n",
    "    train_history['loss'].append(np.mean(epoch_history['loss']))\n",
    "    train_history['acc'].append(np.mean(epoch_history['acc']))\n",
    "    train_history['val_loss'].append(np.mean(epoch_history['val_loss']))\n",
    "    train_history['val_acc'].append(np.mean(epoch_history['val_acc']))\n",
    "    \n",
    "    print(\"***EPOCH SUMMARY*** Loss: {} Acc: {} | Test Loss: {} Test Acc {}\".format(train_history['loss'][-1], train_history['acc'][-1],\n",
    "                                                                                    train_history['val_loss'][-1], train_history['val_acc'][-1]))\n",
    "\n",
    "    if train_history['val_acc'][-1] > best_acc:\n",
    "        best_acc = train_history['val_acc'][-1]\n",
    "        save_path = saver.save(sess, \"./model.ckpt\")\n",
    "        print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training result\n",
    "\n",
    "It's good practice to visualize accuracy and loss evolution. [plotly](https://plot.ly/) library gives as way to build interactive figures inplace in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlefont = dict(family='Courier New, monospace', size=18, color='#7f7f7f')\n",
    "layout = go.Layout(title='Traing & Test loss', xaxis=dict(title='Epoch', titlefont=titlefont),\n",
    "                                    yaxis=dict(title='Loss', titlefont=titlefont))\n",
    "fig = go.Figure(data=[go.Scatter(y=train_history['loss'], name='Train loss'), go.Scatter(y=train_history['val_loss'], name='Test loss')], layout=layout)\n",
    "plotly.offline.iplot(fig)\n",
    "\n",
    "layout = go.Layout(title='Traing & Test accuracy', xaxis=dict(title='Epoch', titlefont=titlefont),\n",
    "                                    yaxis=dict(title='Accuracy', titlefont=titlefont))\n",
    "fig = go.Figure(data=[go.Scatter(y=train_history['acc'], name='Train accuracy'), go.Scatter(y=train_history['val_acc'], name='Test accuracy')], layout=layout)\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance data preparation\n",
    "\n",
    "For futher model analysis we need some data. Very good solution - store all statistic data in pandas DataFrame data structure. Let's evaluate all test samples and store all information about prediction, probabilities and other information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(sess, \"./model.ckpt\")\n",
    "sess.run(test_init_op)\n",
    "confusion_matrix = np.zeros([43,43])\n",
    "test_analys = trainDf.copy()\n",
    "predictions = []\n",
    "probabilities = []\n",
    "analys = []\n",
    "\n",
    "for i in tqdm.tqdm_notebook(range(samples_test//batch_size), ascii=True, desc='Test best model'):\n",
    "    _files, _predictions, _probas, _gts, _cm = sess.run([load_filename, pred_classes, pred_probas, load_label, confusion_matrix_op], feed_dict={dp_rate: 0, manual_load: False, img_placeholder: zero_img, label_placeholder: zero_label})\n",
    "    confusion_matrix += _cm\n",
    "    for i in range(batch_size):\n",
    "        sample_info = {'image': _files[i].decode(), 'prediction': int(_predictions[i]), 'gt': int(_gts[i]), 'gt_probas': _probas[i][_gts[i]],\n",
    "                       'prediction_probas': _probas[i][_predictions[i]], 'prediction_type': 'Correct' if _gts[i] == _predictions[i] else 'Wrong'}\n",
    "        for cls_id, j in enumerate(_probas[i]):\n",
    "            sample_info['prob_{}'.format(cls_id)] = j\n",
    "        analys.append(sample_info)\n",
    "\n",
    "analys_df = pd.DataFrame(analys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation analys overview\n",
    "\n",
    "We have built pandas DataFrame. Let's observe it structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analys_df.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random prediction visualization\n",
    "\n",
    "Using information computed above, we can visualize some random samples with their predictions. As we can see - it is impossible to recognize some pictures definitely by human, but network still generates correct predictions. Awesome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 4\n",
    "fig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(25, 8))\n",
    "visualize = trainDf.sample(rows*cols)\n",
    "\n",
    "analys_df_copy = analys_df.copy()\n",
    "analys_df_copy = analys_df_copy.sample(frac=1)\n",
    "\n",
    "idx = 0\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        img = cv2.imread(analys_df_copy.iloc[idx]['image'])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (100, 100))\n",
    "        \n",
    "        gt = analys_df_copy.iloc[idx]['gt']\n",
    "        pred = analys_df_copy.iloc[idx]['prediction']\n",
    "        \n",
    "        axs[i,j].imshow(img)\n",
    "        axs[i,j].set_title('Predicted: {}\\nGround truth {}'.format(labels[pred], labels[gt]), fontsize=14)\n",
    "        axs[i,j].get_xaxis().set_visible(False)\n",
    "        axs[i,j].get_yaxis().set_visible(False)\n",
    "        idx += 1\n",
    "        \n",
    "fig.suptitle(\"Random prediction\", fontsize=30, y=2.1, x=0.515);\n",
    "plt.subplots_adjust(left=None, bottom=None, right=0.9, top=1.9, wspace=None, hspace=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrong prediction visualization\n",
    "\n",
    "Our model can't achieve perfect accuracy (i.e. 100%). Let's visualize wrong predicted samples. Some of them have realy bad quality, resolution. Others have unexpected artifacts (such as extreame rotation, half hidden signs or shaddow). This situations wasn't present in train part, so network have no idea how to deal with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 4\n",
    "fig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(25, 8))\n",
    "visualize = trainDf.sample(rows*cols)\n",
    "\n",
    "analys_df_copy = analys_df[analys_df['prediction_type'] == 'Wrong'].copy()\n",
    "analys_df_copy = analys_df_copy.sample(frac=1)\n",
    "\n",
    "idx = 0\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        img = cv2.imread(analys_df_copy.iloc[idx]['image'])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (100, 100))\n",
    "        \n",
    "        gt = analys_df_copy.iloc[idx]['gt']\n",
    "        pred = analys_df_copy.iloc[idx]['prediction']\n",
    "        \n",
    "        axs[i,j].imshow(img)\n",
    "        axs[i,j].set_title('Predicted: {}\\nGround truth {}'.format(labels[pred], labels[gt]), fontsize=14)\n",
    "        axs[i,j].get_xaxis().set_visible(False)\n",
    "        axs[i,j].get_yaxis().set_visible(False)\n",
    "        idx += 1\n",
    "        \n",
    "fig.suptitle(\"Wrong prediction\", fontsize=30, y=2.1, x=0.515);\n",
    "plt.subplots_adjust(left=None, bottom=None, right=0.9, top=1.9, wspace=None, hspace=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dump all wrong prediction for further analys by someone else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './output'\n",
    "error_dir = './output/errors'\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "if not os.path.isdir(error_dir):\n",
    "    os.mkdir(error_dir)\n",
    "\n",
    "for idx, row in tqdm.tqdm_notebook(analys_df[analys_df['prediction_type'] == 'Wrong'].iterrows()):\n",
    "    name = os.path.splitext(os.path.basename(row['image']))[0]\n",
    "    name = '{}__{}__as__{}.png'.format(name, labels[row['gt']].replace(' ', '_'), labels[row['prediction']].replace(' ', '_'))\n",
    "    img = cv2.imread(row['image'])\n",
    "    cv2.imwrite(os.path.join(error_dir, name), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create tar archive for convenient way to download all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -cvf ./errors.tar ./output/errors 1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "Confusion matrix gives us additional information about accuracy distribution. It's naturally that network may be confused in prediction 'Pedestrian' and 'Other dangerous' signs. Also, confusion matrix can give us idea what to improove in network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix\n",
    "f = np.sum(cm, axis=1)\n",
    "normalized_cm = cm\n",
    "for i in range(43):\n",
    "    normalized_cm[i, :] /= sum(normalized_cm[i, :])\n",
    "\n",
    "normalized_cm = np.round(normalized_cm, 2)\n",
    "    \n",
    "fig, ax = plt.subplots(1,1, figsize=((20, 20)))\n",
    "\n",
    "ax.imshow(normalized_cm)\n",
    "\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_yticks(np.arange(len(labels)))\n",
    "\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "for i in range(len(labels)):\n",
    "   for j in range(len(labels)):\n",
    "       ax.text(j, i, normalized_cm[i, j], ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title('Confusion matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are histogram of prediction types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, sharex=False, sharey=True, figsize=(25, 7))\n",
    "\n",
    "px = sns.countplot(x='prediction_type', data=analys_df, ax=axs)\n",
    "axs.set_title('Prediction type distribution', fontsize=18)\n",
    "axs.set_xlabel('Prediction type', fontsize=16)\n",
    "axs.set_ylabel('Fraction', fontsize=16);\n",
    "\n",
    "\n",
    "total = analys_df.shape[0]\n",
    "for idx, p in enumerate(px.patches):\n",
    "        px.annotate('{:.1f}%'.format(p.get_height()/total*100), (p.get_x()+0.365, p.get_height()+100), fontsize=18)\n",
    "\n",
    "\n",
    "px.yaxis.set_ticks(np.linspace(0, total, 11))\n",
    "px.set_yticklabels(map('{:.1f}%'.format, 100*px.yaxis.get_majorticklocs()/total));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analys_df.prediction_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerically accuracy by class\n",
    "Confusion matrix produce perfect visualization for understanding why some class is so bad in predictions, but it's no so convenient way to sum up numerrically how this class is good predicted. Below are histgrams of corretly and wrong predicted classes without correlation with other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction_by_class = analys_df[analys_df['prediction_type'] == 'Correct']['gt'].value_counts() / testDf['ClassId'].value_counts().sort_index()\n",
    "correct_prediction_by_class_df = pd.DataFrame({'accuracy': correct_prediction_by_class, 'class': labels})\n",
    "\n",
    "wrong_prediction_by_class = analys_df[analys_df['prediction_type'] == 'Wrong']['gt'].value_counts() / testDf['ClassId'].value_counts().sort_index()\n",
    "wrong_prediction_by_class_df = pd.DataFrame({'error': wrong_prediction_by_class, 'class': labels})\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, sharex=False, sharey=True, figsize=(25, 27))\n",
    "\n",
    "sns.barplot(x='class', y='accuracy', data=correct_prediction_by_class_df, ax=axs[1])\n",
    "sns.barplot(x='class', y='error', data=wrong_prediction_by_class_df, ax=axs[0])\n",
    "\n",
    "axs[0].set_title('Wrong prediction grouped by class', fontsize=18)\n",
    "axs[0].set_xlabel('Class', fontsize=16)\n",
    "axs[0].set_ylabel('Percent of wrong prediction', fontsize=16)\n",
    "axs[0].set_xticklabels(rotation=90, labels=labels)\n",
    "\n",
    "axs[1].set_title('Correct prediction grouped by class', fontsize=18)\n",
    "axs[1].set_xlabel('Class', fontsize=16)\n",
    "axs[1].set_ylabel('Percent of correct prediction', fontsize=16)\n",
    "axs[1].set_xticklabels(rotation=90, labels=labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence analysis\n",
    "\n",
    "Classification neural networks produce not only discrete prediction. Model is also generated probability for each class (provided by softmax layer). Can we break up with wrong prediction by analys confidence levels?\n",
    "\n",
    "Let's build confidence level distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, sharex=False, sharey=True, figsize=(25,12))\n",
    "\n",
    "sns.boxplot(x='prediction_type', y='prediction_probas', data=analys_df, ax=axs);\n",
    "axs.set_title('Prediction probabilities distribution', fontsize=18);\n",
    "axs.set_xlabel('Predict type', fontsize=16)\n",
    "axs.set_ylabel('Probability distribution', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence level threshold\n",
    "\n",
    "Find threshold for confidence level by experiment. All predictions under this threshold will be threated as 'No shure'. It is clearly, that we can't achive more accuracy that we had before, but we can reduce wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, sharex=False, sharey=True, figsize=(25,10))\n",
    "\n",
    "confidence_thresholds = iter([0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        confidence_threshold = next(confidence_thresholds)\n",
    "        \n",
    "        analys_df_confidence = analys_df.copy()\n",
    "        new_predict = []\n",
    "        for idx, row in analys_df_confidence.iterrows():\n",
    "            new_predict.append('Not sure' if row['prediction_probas'] < confidence_threshold else 'Probably correct')\n",
    "\n",
    "        analys_df_confidence['confidence_analys'] = new_predict\n",
    "        \n",
    "        axs = axes[i,j]\n",
    "        \n",
    "        px = sns.countplot(x='prediction_type', hue='confidence_analys', data=analys_df_confidence, ax=axs);\n",
    "        axs.set_title('Confidence threshold {}'.format(confidence_threshold), fontsize=18)\n",
    "        axs.set_xlabel('Prediction type', fontsize=16)\n",
    "        axs.set_ylabel('Fraction', fontsize=16);\n",
    "        axs.legend(title='Confidence prediction')\n",
    "\n",
    "        total = analys_df_confidence.shape[0]\n",
    "        for idx, p in enumerate(px.patches):\n",
    "            px.annotate('{:.1f}%'.format(p.get_height()/total*100), (p.get_x()+0.14, p.get_height()+100), fontsize=18)\n",
    "\n",
    "\n",
    "        px.yaxis.set_ticks(np.linspace(0, total, 11))\n",
    "        px.set_yticklabels(map('{:.1f}%'.format, 100*px.yaxis.get_majorticklocs()/total));\n",
    "\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=1.2, wspace=None, hspace=None)\n",
    "fig.suptitle('Confidence analys', fontsize=20, y=1.3, x=0.51);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more clearly answer, let's approximate function of confidence level threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidences = []\n",
    "\n",
    "for confidence_threshold in tqdm.tqdm_notebook(np.arange(start=0, stop=1.1, step=0.01)):\n",
    "\n",
    "    analys_df_confidence = analys_df.copy()\n",
    "    new_predict = []\n",
    "    for idx, row in analys_df_confidence.iterrows():\n",
    "        new_predict.append('Not sure' if row['prediction_probas'] < confidence_threshold else 'Probably correct')\n",
    "\n",
    "    analys_df_confidence['confidence_analys'] = new_predict\n",
    "\n",
    "    false_positive = analys_df_confidence[(analys_df_confidence['prediction_type'] == 'Wrong') &\n",
    "                                          (analys_df_confidence['confidence_analys'] == 'Probably correct')].shape[0]/analys_df_confidence.shape[0]\n",
    "    \n",
    "    true_positive = analys_df_confidence[(analys_df_confidence['prediction_type'] == 'Correct') &\n",
    "                                          (analys_df_confidence['confidence_analys'] == 'Probably correct')].shape[0]/analys_df_confidence.shape[0]\n",
    "    \n",
    "    not_sure_at_wrong = analys_df_confidence[(analys_df_confidence['prediction_type'] == 'Wrong') &\n",
    "                                          (analys_df_confidence['confidence_analys'] == 'Not sure')].shape[0]/analys_df_confidence.shape[0]\n",
    "    \n",
    "    not_sure_at_correct = analys_df_confidence[(analys_df_confidence['prediction_type'] == 'Correct') &\n",
    "                                          (analys_df_confidence['confidence_analys'] == 'Not sure')].shape[0]/analys_df_confidence.shape[0]\n",
    "    \n",
    "    cf_level_result = {'fp': false_positive, 'tp': true_positive, 'ns_w': not_sure_at_wrong, 'ns_c': not_sure_at_correct, 'cf': confidence_threshold}\n",
    "    confidences.append(cf_level_result)\n",
    "    \n",
    "confidences = pd.DataFrame(confidences)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, sharex=False, sharey=True, figsize=(25,10))\n",
    "\n",
    "\n",
    "sns.lineplot(x='cf', y='fp', data=confidences, ax=axes, label='False positive');\n",
    "sns.lineplot(x='cf', y='tp', data=confidences, ax=axes, label='True positive');\n",
    "sns.lineplot(x='cf', y='ns_c', data=confidences, ax=axes, label='Not shure at correct prediction');\n",
    "sns.lineplot(x='cf', y='ns_w', data=confidences, ax=axes, label='Not shure at wrong prediction');\n",
    "\n",
    "axes.set_yscale('log')\n",
    "axes.set_xlabel('Confidence threshold', fontsize=16)\n",
    "axes.set_ylabel('Fraction of prediction', fontsize=16);\n",
    "axes.legend(loc='center left', prop={'size': 16})\n",
    "fig.suptitle('Confidence threshold analys', fontsize=20);\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution neural network visualization\n",
    "\n",
    "There are many method for consolutional neural network visualization in many papers. The simplest one - visualize image after consolutional layer. We stores all results after each convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "res = sess.run([load_img, load_label, conv1, conv2, conv3, conv4, conv5, conv6],\n",
    "               feed_dict={dp_rate: 0,\n",
    "                          manual_load: False,\n",
    "                          img_placeholder: np.zeros([12,60,60,3])})\n",
    "img, label = res[0], res[1]\n",
    "img_visible = (img*255).astype(np.uint8)[0, :, :, :]\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "ax.imshow(img_visible)\n",
    "ax.set_title(labels[label[0]])\n",
    "\n",
    "filters = res[2:]\n",
    "layer_names = ['Convolutional layer {}'.format(x+1) for x in range(len(filters))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layers have many filters. Visualizing all images is very epxencive and hard, but we will try. Results after convulition is not normalized, so we will normalized it manualy. Visualization looks like heatmap but it is not heatmap. It's just pretty visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filter_index in range(len(filters)):\n",
    "    layers = filters[filter_index]\n",
    "    filter_count = layers.shape[3]\n",
    "    n_columns = 6\n",
    "    n_rows = math.ceil(filter_count / n_columns) + 1\n",
    "    fig = plt.figure(figsize=(24,n_rows*4));\n",
    "    fig.suptitle(layer_names[filter_index], fontsize=16)\n",
    "    for i in range(filter_count):\n",
    "        plt.subplot(n_rows, n_columns, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.title('Filter: {0} '.format(str(i)))\n",
    "        plt.imshow(layers[0,:,:,i], interpolation=\"nearest\", cmap='bwr')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM\n",
    "\n",
    "Another good way to understand what our model have learned - using Grad-CAM method. [Grad-CAM](https://arxiv.org/abs/1610.02391) - is Gradient-weighted Class Activation Map. We can see which parts of image are important for prediction (i.e. mostly cause prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(sess, layer, predicted_class, nb_classes, img):\n",
    "    conv_layer = layer\n",
    "    one_hot = tf.sparse_to_dense(predicted_class, [nb_classes], 1.0)\n",
    "    signal = tf.multiply(logits, one_hot)\n",
    "    loss = tf.reduce_mean(signal)\n",
    "    \n",
    "    grads = tf.gradients(loss, conv_layer)[0]\n",
    "    # Normalizing the gradients\n",
    "    norm_grads = tf.div(grads, tf.sqrt(tf.reduce_mean(tf.square(grads))) + tf.constant(1e-5))\n",
    "\n",
    "    output, grads_val = sess.run([conv_layer, norm_grads], feed_dict={dp_rate: 0, manual_load: True,\n",
    "                                                                     img_placeholder: img})\n",
    "    output = output[0]           \n",
    "    grads_val = grads_val[0]     \n",
    "\n",
    "    weights = np.mean(grads_val, axis = (0, 1))\n",
    "    cam = np.ones(output.shape[0 : 2], dtype = np.float32)\n",
    "\n",
    "    # Taking a weighted average\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * output[:, :, i]\n",
    "\n",
    "    # Passing through ReLU\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / np.max(cam)\n",
    "    cam = cv2.resize(cam, (224,224))\n",
    "\n",
    "    # Converting grayscale to 3-D\n",
    "    cam3 = np.expand_dims(cam, axis=2)\n",
    "    cam3 = np.tile(cam3,[1,1,3])\n",
    "\n",
    "    return cam3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eveluate some samples and visualize Grad-CAM heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_img, _lb = sess.run([load_img, load_label])\n",
    "grad_ipt = _img.copy()\n",
    "\n",
    "samples = _img.shape[0]//2\n",
    "fig, ax = plt.subplots(ncols=samples, nrows=2, figsize=(20, 8))\n",
    "for i in range(samples):\n",
    "    ax[0, i].imshow(_img[i, :, :, :])\n",
    "    ax[0, i].get_xaxis().set_visible(False)\n",
    "    ax[0, i].get_yaxis().set_visible(False)\n",
    "    ax[0, i].set_title(labels[_lb[i]])\n",
    "    \n",
    "    image = _img[i, :, :, :]\n",
    "    \n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "\n",
    "    grad_ipt[0, :, :] = _img[i, :, :]\n",
    "    heat_map = grad_cam(sess, conv1, _lb[i], 43, grad_ipt)\n",
    "    # resize heat map\n",
    "    heat_map_resized = cv2.resize(heat_map, (height, width))\n",
    "\n",
    "    # normalize heat map\n",
    "    max_value = np.max(heat_map_resized)\n",
    "    min_value = np.min(heat_map_resized)\n",
    "    normalized_heat_map = (heat_map_resized - min_value) / (max_value - min_value)\n",
    "    normalized_heat_map = cv2.applyColorMap((normalized_heat_map*255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    \n",
    "    ax[1, i].imshow(normalized_heat_map)\n",
    "    ax[1, i].get_xaxis().set_visible(False)\n",
    "    ax[1, i].get_yaxis().set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a0ca82fe72f7b6dcc59f88ce1601f3112cb58c024b4d57901a90c4fbc3fa1c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
